{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12d18ed-8dce-4d40-ab45-5b9602feb81b",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc84ce75-fdd4-464e-a95c-1c0dce54926c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda 23.10.0 requires ruamel-yaml<0.18,>=0.11.14, but you have ruamel-yaml 0.18.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m[2024-07-08 00:35:19,852] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n",
      "11.8\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:35:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n",
      "2024-07-08 00:35:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./PPTPhi3/runs/Jul08_00-35-20_jupyter-lrodriguezmartinez-40hof-2duniversity-2ede,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./PPTPhi3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./PPTPhi3,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.2,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-07-08 00:35:20 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules='all-linear', lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 00:35:20,935 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 00:35:21,093 >> loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 00:35:21,095 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:35:21 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "2024-07-08 00:35:21 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3556] 2024-07-08 00:35:21,292 >> loading weights file model.safetensors from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1531] 2024-07-08 00:35:21,296 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-08 00:35:21,297 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8889920aead04eeebaac6b069b71d3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4364] 2024-07-08 00:35:29,834 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-08 00:35:29,836 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-128k-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:955] 2024-07-08 00:35:29,976 >> loading configuration file generation_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-08 00:35:29,977 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 00:35:30,325 >> loading file tokenizer.model from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 00:35:30,326 >> loading file tokenizer.json from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 00:35:30,327 >> loading file added_tokens.json from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 00:35:30,327 >> loading file special_tokens_map.json from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 00:35:30,328 >> loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-08 00:35:30,406 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ppapi (/home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.builder - Found cached dataset ppapi (/home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.info - Loading Dataset info from /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-01293d9a91e4fb71.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-01293d9a91e4fb71.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-20cfd73d90208a6c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-20cfd73d90208a6c.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-20cfd73d90208a6c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-20cfd73d90208a6c.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:2048] 2024-07-08 00:36:01,878 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1751] 2024-07-08 00:36:01,895 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_num_proc. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[INFO|training_args.py:2048] 2024-07-08 00:36:01,899 >> PyTorch: setting up devices\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Process #0 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00000_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #0 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00000_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00001_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #1 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00001_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00002_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #2 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00002_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00003_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #3 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00003_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 4 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00000_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #0 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00000_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00001_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #1 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00001_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00002_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #2 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00002_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00003_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Process #3 will write at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_00003_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/Leonardorm7___ppapi/default/0.0.0/191557e2472819f4421c903a69545fcac95123ea/cache-402670b64006129f_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 4 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:01 - INFO - datasets.arrow_dataset - Concatenating 4 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|trainer.py:592] 2024-07-08 00:36:06,466 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:642] 2024-07-08 00:36:06,468 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-08 00:36:06,679 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-08 00:36:06,680 >>   Num examples = 201\n",
      "[INFO|trainer.py:2130] 2024-07-08 00:36:06,681 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2131] 2024-07-08 00:36:06,681 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-08 00:36:06,682 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-08 00:36:06,682 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2136] 2024-07-08 00:36:06,683 >>   Total optimization steps = 60\n",
      "[INFO|trainer.py:2137] 2024-07-08 00:36:06,684 >>   Number of trainable parameters = 3,821,079,552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 00:36:06 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:49, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 00:38:09,881 >> Saving model checkpoint to outputs/checkpoint-60\n",
      "[INFO|configuration_utils.py:472] 2024-07-08 00:38:09,886 >> Configuration saved in outputs/checkpoint-60/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-08 00:38:09,887 >> Configuration saved in outputs/checkpoint-60/generation_config.json\n",
      "[INFO|modeling_utils.py:2698] 2024-07-08 00:38:27,577 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at outputs/checkpoint-60/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 00:38:27,580 >> tokenizer config file saved in outputs/checkpoint-60/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 00:38:27,582 >> Special tokens file saved in outputs/checkpoint-60/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-08 00:38:58,638 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:3788] 2024-07-08 00:38:58,658 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-08 00:38:58,658 >>   Num examples = 201\n",
      "[INFO|trainer.py:3793] 2024-07-08 00:38:58,659 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     4.7059\n",
      "  total_flos               =  8272789GF\n",
      "  train_loss               =     0.2519\n",
      "  train_runtime            = 0:02:51.95\n",
      "  train_samples_per_second =      5.583\n",
      "  train_steps_per_second   =      0.349\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 00:39:02,862 >> Saving model checkpoint to ./PPTPhi3\n",
      "[INFO|configuration_utils.py:472] 2024-07-08 00:39:02,866 >> Configuration saved in ./PPTPhi3/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-08 00:39:02,868 >> Configuration saved in ./PPTPhi3/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =     4.7059\n",
      "  eval_loss               =     0.1668\n",
      "  eval_runtime            = 0:00:04.19\n",
      "  eval_samples            =        201\n",
      "  eval_samples_per_second =     47.903\n",
      "  eval_steps_per_second   =      6.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2698] 2024-07-08 00:39:20,058 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./PPTPhi3/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 00:39:20,061 >> tokenizer config file saved in ./PPTPhi3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 00:39:20,063 >> Special tokens file saved in ./PPTPhi3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 00:39:20,139 >> tokenizer config file saved in ./PPTPhi3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 00:39:20,141 >> Special tokens file saved in ./PPTPhi3/special_tokens_map.json\n",
      "[INFO|configuration_utils.py:472] 2024-07-08 00:39:20,236 >> Configuration saved in ./PPTPhi3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fine-Tuned and Saved\n"
     ]
    }
   ],
   "source": [
    "# Installation of necessary libraries\n",
    "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip -q install bitsandbytes\n",
    "!pip -q install peft transformers trl datasets\n",
    "!pip -q install deepspeed\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys\n",
    "import logging\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "# CUDA installation verification\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Hyperparameter settings\n",
    "training_config = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"learning_rate\": 5.0e-06,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./PPTPhi3\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "}\n",
    "\n",
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}\n",
    "\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)\n",
    "\n",
    "# logging configuration\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Parameter log\n",
    "logger.warning(\n",
    "    f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "    + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "logger.info(f\"PEFT parameters {peft_conf}\")\n",
    "\n",
    "# Model loading and tokenizer\n",
    "checkpoint_path = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 512\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# Data processing\n",
    "# dataset = load_dataset(\"Leonardorm7/PP\") #Dataset1\n",
    "dataset = load_dataset(\"Leonardorm7/PPAPI\") #Dataset2\n",
    "\n",
    "def preprocessing(sample):\n",
    "    system_prompt_template = \"\"\"<s>\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction: <<user_question>>\n",
    "### Response:\n",
    "<<user_response>>\n",
    "</s>\n",
    "\"\"\"\n",
    "    user_message = sample['Input']\n",
    "    user_response = sample['Python code']\n",
    "    prompt_template = system_prompt_template.replace(\"<<user_question>>\", user_message).replace(\"<<user_response>>\", user_response)\n",
    "    return {\"train\": prompt_template}\n",
    "\n",
    "training_dataset = dataset.map(preprocessing)\n",
    "train_dataset = training_dataset['train'].map(lambda x: x, batched=True)\n",
    "\n",
    "# Create a test data set\n",
    "test_dataset = training_dataset['train'].map(lambda x: x, batched=True)\n",
    "\n",
    "# Training\n",
    "fp16_support = torch.cuda.is_available() and not torch.cuda.is_bf16_supported()\n",
    "bf16_support = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    dataset_text_field=\"train\",\n",
    "    max_seq_length=512,\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=fp16_support,\n",
    "        bf16=bf16_support,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# Evaluation\n",
    "tokenizer.padding_side = 'left'\n",
    "metrics = trainer.evaluate()\n",
    "metrics[\"eval_samples\"] = len(test_dataset)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(train_conf.output_dir)\n",
    "tokenizer.save_pretrained(train_conf.output_dir)\n",
    "model.config.save_pretrained(train_conf.output_dir)\n",
    "print('Model Fine-Tuned and Saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584161a-e60f-4086-8969-797e5ec6b574",
   "metadata": {},
   "source": [
    "# Trained model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c080ca93-2da6-4208-a05e-27cc6de8d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:328] 2024-07-08 00:39:20,253 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Create a 5-slide presentation with yellow background, each slide with Slide Title in Calibri size 45 and a bullet list containing four points in Calibri size 20\n",
      "### Instruction: Create a 5 slide presentation with yellow background, each slide with 'Slide Title' in Calibri size 45 and a bullet list containing four points in Calibri size 20\n",
      "### Response:\n",
      "api.create_presentation('Yellow Slides')\n",
      "\n",
      "titles = [\"Slide 1\", \"Slide 2\", \"Slide 3\", \"Slide 4\", \"Slide 5\"]\n",
      "bullet_points = [\n",
      "    [\"Point 1\", \"Point 2\", \"Point 3\", \"Point 4\"],\n",
      "    [\"Item A\", \"Item B\", \"Item C\", \"Item D\"],\n",
      "    [\"Fact 1\", \"Fact 2\", \"Fact 3\", \"Fact 4\"],\n",
      "    [\"Detail X\", \"Detail Y\", \"Detail Z\", \"Detail W\"],\n",
      "    [\"Insight 1\", \"Insight 2\", \"Insight 3\", \"Insight 4\"]\n",
      "]\n",
      "\n",
      "for i in range(5):\n",
      "    api.add_slide(layout='title and content')\n",
      "    api.change_background_color(i + 1, (255, 255, 0))\n",
      "    api.add_text_to_slide(i + 1, titles[i], placeholder=0)\n",
      "    for point in bullet_points[i]:\n",
      "        api.add_bullet_point(i + 1, point)\n",
      "    api.change_font(i + 1, 0, 'Calibri')  \n",
      "    api.change_font_size(i + 1, 0, 45)  \n",
      "    api.change_font(i + 1, 1, 'Calibri')  \n",
      "    api.change_font_size(i + 1, 1, 20)  \n",
      "api.save_presentation('yellow_slides.pptx')\n",
      "</s>\n",
      "### Instruction: Create a 5 slide presentation with yellow background, each slide with 'Slide Title' in Calibri size 45 and a bullet list containing four points in Calibri size 20\n",
      "### Response:\n",
      "api.create_presentation('Yellow Slides')\n",
      "\n",
      "titles = [\"Slide 1\", \"Slide 2\", \"Slide 3\", \"Slide 4\", \"Slide 5\"]\n",
      "bullet_points = [\n",
      "    [\"Point 1\", \"Point 2\", \"Point 3\", \"Point 4\"],\n",
      "    [\"Item A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_response(input_text, max_length=600):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "#*******************************Put your prompt here******************************************\n",
    "\n",
    "input_text='Create a 5-slide presentation with yellow background, each slide with Slide Title in Calibri size 45 and a bullet list containing four points in Calibri size 20'\n",
    "response = generate_response(input_text)\n",
    "\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048eee07-1267-4672-b409-0c2dfb1daacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f977f3-266c-4f91-a89e-760638fa6e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
